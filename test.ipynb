{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_adpative_margin(batch_size:int,sort:bool=True):\n",
    "    samples = torch.normal(0.3, 0.01**0.05, size=(batch_size,))\n",
    "    samples = torch.min(torch.max(samples, torch.tensor(0.0001)), torch.tensor(0.5))\n",
    "    if sort:\n",
    "        sorted_samples = torch.sort(samples)[0]\n",
    "    return sorted_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=sample_adpative_margin(10,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e-04, 1.0000e-04, 1.0000e-04, 3.7814e-01, 5.0000e-01, 5.0000e-01,\n",
       "        5.0000e-01, 5.0000e-01, 5.0000e-01, 5.0000e-01])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1033, 0.2439, 0.2554, 0.2646, 0.3172, 0.3488, 0.4003, 0.4103, 0.4357,\n",
       "        0.4391])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the mean and variance\n",
    "mean = 0.3\n",
    "variance = 0.01\n",
    "\n",
    "# Sample 10 values from the normal distribution\n",
    "samples = torch.normal(mean, variance**0.5, size=(10,))\n",
    "\n",
    "# Sort the samples in ascending order\n",
    "sorted_samples = torch.sort(samples).values\n",
    "\n",
    "sorted_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "# 假设`tokenizer`是一个已经加载的BERT分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('/sharedata/models/Bert-Base-Chinese')\n",
    "\n",
    "# 假设`batch_sentences`是一个包含多个句子的列表\n",
    "batch_sentences = [[[\"你好，世界！\", \"BERT模型是强大的。\"],\n",
    "                   [\"你好，世界！\", \"我最想打你了。\"]],\n",
    "                   \n",
    "                   [[\"我很好\",\"你是谁\"],\n",
    "                    [\"我很好\",\"你撒大苏打\"]],\n",
    "                     [[\"我很好\",\"你是谁\"],\n",
    "                    [\"我很好\",\"你撒大苏打\"]],\n",
    "                     [[\"我很好\",\"你是谁\"],\n",
    "                    [\"我很好\",\"你撒大苏打\"]]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CD(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer =tokenizer\n",
    "        self.data = batch_sentences\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.tokenizer(self.data[index],padding=\"max_length\", truncation=True, return_tensors='pt',max_length=16)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "cd = CD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl =DataLoader(cd,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 101,  872, 1962, 8024,  686, 4518, 8013,  102,  100, 3563, 1798,\n",
      "          3221, 2487, 1920, 4638,  102],\n",
      "         [ 101,  872, 1962, 8024,  686, 4518, 8013,  102, 2769, 3297, 2682,\n",
      "          2802,  872,  749,  511,  102]],\n",
      "\n",
      "        [[ 101, 2769, 2523, 1962,  102,  872, 3221, 6443,  102,    0,    0,\n",
      "             0,    0,    0,    0,    0],\n",
      "         [ 101, 2769, 2523, 1962,  102,  872, 3054, 1920, 5722, 2802,  102,\n",
      "             0,    0,    0,    0,    0]]])\n"
     ]
    }
   ],
   "source": [
    "for data in dl:\n",
    "    print(data.input_ids)\n",
    "    print(\"================================\")\n",
    "    print(data.input_ids.reshape())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['你好，世界！', 'BERT模型是强大的。'], ['你好，世界！', '我最想打你了。']]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize, add special tokens, padding, attention masks and segment ids\n",
    "encoded_input = tokenizer(batch_sentences[0], padding=\"max_length\", truncation=True, return_tensors='pt',max_length=64,add_special_tokens=True)\n",
    "\n",
    "# encoded_input包含以下键：\n",
    "# input_ids: token的ID\n",
    "# attention_mask: 指示哪些是填充token\n",
    "# token_type_ids: 句子片段的ID（对于单句输入，通常是全0）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101,  872, 1962, 8024,  686, 4518, 8013,  102,  100, 3563, 1798, 3221,\n",
       "         2487, 1920, 4638,  511,  102,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [ 101,  872, 1962, 8024,  686, 4518, 8013,  102, 2769, 3297, 2682, 2802,\n",
       "          872,  749,  511,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tokenizer(batch_sentences[0], padding=\"max_length\", truncation=True,max_length=64,add_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "result.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 你 好 ， 世 界 ！ [SEP] [UNK] 模 型 是 强 大 的 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(result.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['你好，世界！', 'BERT模型是强大的。'], ['你好，世界！', '我最想打你了。']]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
